# Spark + Kafka Streaming Project

This project demonstrates real-time data streaming from Kafka, processed using Apache Spark Structured Streaming. It provides a simple Kafka producer and a Spark streaming script to consume and process messages in real-time.

## ğŸ“„ Table of Contents

- [Spark + Kafka Streaming Project](#spark--kafka-streaming-project)
  - [ğŸ“„ Table of Contents](#-table-of-contents)
  - [ğŸ“ Overview](#-overview)
  - [ğŸ—‚ï¸ Project Structure](#ï¸-project-structure)
  - [âš™ï¸ Requirements](#ï¸-requirements)
  - [ğŸš€ Setup Instructions](#-setup-instructions)
    - [1. Start Kafka and Zookeeper](#1-start-kafka-and-zookeeper)
    - [2. Run the Kafka Producer](#2-run-the-kafka-producer)
    - [3. Run the Spark Streaming Script](#3-run-the-spark-streaming-script)
  - [ğŸ› ï¸ Customization](#ï¸-customization)
  - [ğŸ©º Troubleshooting](#-troubleshooting)
- [ğŸ“¦ **Há»‡ thá»‘ng gá»“m 2 pháº§n chÃ­nh:**](#-há»‡-thá»‘ng-gá»“m-2-pháº§n-chÃ­nh)
  - [ğŸ¯ **Producer gá»­i dá»¯ liá»‡u gÃ¬?**](#-producer-gá»­i-dá»¯-liá»‡u-gÃ¬)
  - [ğŸ” **Spark Structured Streaming Ä‘á»c vÃ  xá»­ lÃ½ gÃ¬?**](#-spark-structured-streaming-Ä‘á»c-vÃ -xá»­-lÃ½-gÃ¬)
  - [ğŸ“ **TÃ³m táº¯t Workflow:**](#-tÃ³m-táº¯t-workflow)

---

## ğŸ“ Overview

- **Kafka** acts as the message broker, receiving and storing streaming messages (e.g., from the sample producer).
- **Apache Spark Structured Streaming** consumes and processes these messages in real time.

---

## ğŸ—‚ï¸ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ kafka/                 # Kafka producer/consumer code
â”‚   â””â”€â”€ producer.py
â”œâ”€â”€ spark/                 # Spark streaming code
â”‚   â””â”€â”€ spark_streaming.py
â”œâ”€â”€ configs/               # Sample Kafka/Spark config files
â”œâ”€â”€ scripts/               # Utilities to start/stop services, create topics
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This file
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Requirements

- Python 3.8+
- Apache Kafka (locally installed or Docker)
- Apache Spark 3.x (with PySpark)
- Python packages in `requirements.txt`:
  - `pyspark`
  - `kafka-python`

**Install Python dependencies:**
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Setup Instructions

### 1. Start Kafka and Zookeeper

If you're using Docker Compose (recommended for cp-kafka):

```bash
docker-compose up -d
```
- Ensure that ports and advertised listeners in `docker-compose.yml` match your scripts and producer.

To shut down:
```bash
docker-compose down
```


### 2. Run the Kafka Producer
Open a new terminal:
```bash
python kafka/producer.py
```

### 3. Run the Spark Streaming Script
Open another terminal:
```bash
python spark/spark_streaming.py
```

---

## ğŸ› ï¸ Customization

- To change the Kafka topic, update the topic name in both the producer and Spark scripts.
- For advanced transformations, edit `spark_streaming.py` (e.g., process data, run analytics).

---

## ğŸ©º Troubleshooting

- **Kafka Errors:** Make sure Zookeeper and Kafka are both running before starting producer or Spark jobs.
- **Connection Issues:** Ensure `localhost:9092` (or configured broker) is correct.
- **Dependency Errors:** Reinstall requirements via `pip install -r requirements.txt`.

---

# ğŸ“¦ **Há»‡ thá»‘ng gá»“m 2 pháº§n chÃ­nh:**
1. **Kafka Producer:** Sinh vÃ  gá»­i dá»¯ liá»‡u sensor (JSON) lÃªn Kafka topic.
2. **Spark Structured Streaming:** Láº¥y dá»¯ liá»‡u tá»« Kafka, parse/hiá»ƒn thá»‹/tiá»n xá»­ lÃ½ trÃªn console.

---

## ğŸ¯ **Producer gá»­i dá»¯ liá»‡u gÃ¬?**  
**(What does the producer send?)**

Producer liÃªn tá»¥c sinh ra cÃ¡c báº£n ghi dáº¡ng JSON Ä‘áº¡i diá»‡n cho giÃ¡ trá»‹ Ä‘o tá»« nhiá»u sensor mÃ´i trÆ°á»ng, gá»“m cÃ¡c trÆ°á»ng sau:

```json
{
  "sensor_id": "1",
  "temperature": 30.5,
  "humidity": 55.2,
  "pressure": 1012.4,
  "pm25": 40.3,
  "pm10": 65.1,
  "co2": 900.0
}
```
- **Giáº£i thÃ­ch:**
  - `sensor_id`: ID cá»§a thiáº¿t bá»‹ sensor (chuá»—i sá»‘).
  - `temperature`: Nhiá»‡t Ä‘á»™ (Â°C), giÃ¡ trá»‹ thá»±c.
  - `humidity`: Äá»™ áº©m (%), giÃ¡ trá»‹ thá»±c.
  - `pressure`: Ãp suáº¥t (hPa), giÃ¡ trá»‹ thá»±c.
  - `pm25`, `pm10`: Chá»‰ sá»‘ bá»¥i má»‹n (Âµg/mÂ³).
  - `co2`: HÃ m lÆ°á»£ng CO2 trong khÃ´ng khÃ­ (ppm).

**Producer sáº½ gá»­i má»™t báº£n ghi má»›i lÃªn Kafka topic `sensor-data-full` má»—i giÃ¢y.**

---

## ğŸ” **Spark Structured Streaming Ä‘á»c vÃ  xá»­ lÃ½ gÃ¬?**  
**(What does Spark streaming consume and process?)**

- **Spark Ä‘á»c trá»±c tiáº¿p cÃ¡c báº£n ghi tá»« Kafka topic** `sensor-data-full`.
- **Parse dá»¯ liá»‡u JSON** thÃ nh cÃ¡c cá»™t/kiá»ƒu dá»¯ liá»‡u Spark.
- **Hiá»ƒn thá»‹ káº¿t quáº£ tá»«ng batch lÃªn mÃ n hÃ¬nh console** á»Ÿ dáº¡ng báº£ng rÃµ rÃ ng ("pretty").

VÃ­ dá»¥ káº¿t quáº£ console:
```
+---------+-----------+--------+--------+-----+-----+------+
|sensor_id|temperature|humidity|pressure|pm25|pm10| co2  |
+---------+-----------+--------+--------+-----+-----+------+
|   1     |   31.0    |  52.3  | 1009.7 | 10.4|22.3| 630  |
+---------+-----------+--------+--------+-----+-----+------+
|   3     |   28.7    |  60.1  | 1003.5 | 15.1|23.1| 800  |
+---------+-----------+--------+--------+-----+-----+------+
```

**(Spark does NOT yet store, aggregate, or transform furtherâ€”it just parses and prints.  
Báº¡n cÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘á»ƒ ghi xuá»‘ng DB, tÃ­nh toÃ¡n trung bÃ¬nh, phÃ¡t hiá»‡n báº¥t thÆ°á»ng, v.v. theo nhu cáº§u.)**

---

## ğŸ“ **TÃ³m táº¯t Workflow:**

1. **Start Kafka/Zookeeper container báº±ng Docker Compose**
2. **Start cáº£ producer vÃ  spark consumer  
   báº±ng lá»‡nh:**  
   ```bash
   ./scripts/start_all.sh
   ```
3. **Producer phÃ¡t dá»¯ liá»‡u sensor lÃªn Kafka**  
4. **Spark Ä‘á»c, parse, vÃ  hiá»ƒn thá»‹ sensor lÃªn cá»­a sá»• console**

---
